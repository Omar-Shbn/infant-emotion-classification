{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee06187",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "299ed9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b57cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = Path(r\"C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\images_renamed\")   # folder with  images\n",
    "CSV_PATH   = Path(r\"C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\TIF_labels.xlsx\")  # CSV with columns: image,label\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\")   # where splits/CSVs (and optional folders) go\n",
    "\n",
    "# split ratios\n",
    "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.70, 0.15, 0.15  # must sum to 1\n",
    "\n",
    "# If your CSV image IDs don't include extensions, weâ€™ll search these:\n",
    "ALLOWED_EXTS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"]\n",
    "\n",
    "# If original filenames have suffixes (e.g., \"...-4317HA\") but your CSV has \"...-4317\",\n",
    "# set this to True to allow prefix matches when exact matches fail:\n",
    "ALLOW_PREFIX_MATCH = True\n",
    "\n",
    "# If True, also copy files into ImageNet-style folders data/{split}/{label}/img.jpg\n",
    "MAKE_CLASS_FOLDERS = False\n",
    "# ======================\n",
    "\n",
    "# Standardize class names (case-insensitive) to your 7 classes\n",
    "CANONICAL = {\n",
    "    \"happy\": \"happy\",\n",
    "    \"sad\": \"sad\",\n",
    "    \"anger\": \"anger\",\n",
    "    \"angry\": \"anger\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"disgust\": \"disgust\",\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"surprised\": \"surprise\",\n",
    "    \"fear\": \"fear\",\n",
    "    \"scared\": \"fear\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0883e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(x: str) -> str:\n",
    "    key = re.sub(r\"\\s+\", \"\", str(x).strip().lower())\n",
    "    if key not in CANONICAL:\n",
    "        raise ValueError(f\"Unknown label '{x}'. Add a mapping in CANONICAL.\")\n",
    "    return CANONICAL[key]\n",
    "\n",
    "def resolve_image_path(img_id: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    If img_id has an extension, try directly.\n",
    "    Else try each ALLOWED_EXTS.\n",
    "    If still not found and ALLOW_PREFIX_MATCH=True, accept any file whose stem startswith(img_id).\n",
    "    \"\"\"\n",
    "    cand = IMAGES_DIR / img_id\n",
    "    if cand.suffix:\n",
    "        return cand if cand.exists() else None\n",
    "\n",
    "    # try exact stem with allowed extensions\n",
    "    for ext in ALLOWED_EXTS:\n",
    "        p = IMAGES_DIR / f\"{img_id}{ext}\"\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    if ALLOW_PREFIX_MATCH:\n",
    "        # find files whose stem starts with img_id\n",
    "        matches = [p for p in IMAGES_DIR.iterdir() if p.is_file() and p.stem.startswith(img_id)]\n",
    "        if len(matches) == 1:\n",
    "            return matches[0]\n",
    "        # if multiple matches, prefer allowed extensions\n",
    "        pruned = [m for m in matches if m.suffix.lower() in ALLOWED_EXTS]\n",
    "        if len(pruned) == 1:\n",
    "            return pruned[0]\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9068d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_CLASSES = [\"happy\", \"sad\", \"anger\", \"neutral\", \"disgust\", \"surprise\", \"fear\"]\n",
    "CLASS_TO_ID = {c: i for i, c in enumerate(ORDERED_CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e243063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ORDERED_CLASSES = [\"happy\", \"sad\", \"anger\", \"neutral\", \"disgust\", \"surprise\", \"fear\"]\n",
    "CANONICAL = {\n",
    "    \"happy\":\"happy\",\"sad\":\"sad\",\"anger\":\"anger\",\"angry\":\"anger\",\n",
    "    \"neutral\":\"neutral\",\"disgust\":\"disgust\",\n",
    "    \"surprise\":\"surprise\",\"surprised\":\"surprise\",\n",
    "    \"fear\":\"fear\",\"scared\":\"fear\"\n",
    "}\n",
    "CLASS_TO_ID = {c:i for i,c in enumerate(ORDERED_CLASSES)}\n",
    "\n",
    "def normalize_label(x: str) -> str:\n",
    "    key = re.sub(r\"\\s+\", \"\", str(x).strip().lower())\n",
    "    if key not in CANONICAL:\n",
    "        raise ValueError(f\"Unknown label '{x}'\")\n",
    "    return CANONICAL[key]\n",
    "\n",
    "def read_labels_safely(path):\n",
    "    path = Path(path)\n",
    "\n",
    "    # 1) If it's actually an Excel workbook (xlsx/xls are ZIPs), use read_excel\n",
    "    if path.suffix.lower() in {\".xlsx\", \".xls\"} or zipfile.is_zipfile(path):\n",
    "        df = pd.read_excel(path, usecols=[0, 1])\n",
    "        df.columns = [\"image\", \"label\"]\n",
    "    else:\n",
    "        # 2) Robust CSV read (handles weird encodings/quotes)\n",
    "        tried = []\n",
    "        for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
    "            for quoting in (csv.QUOTE_MINIMAL, csv.QUOTE_NONE):\n",
    "                try:\n",
    "                    df = pd.read_csv(\n",
    "                        path,\n",
    "                        engine=\"python\",\n",
    "                        sep=None,             # auto-detect delimiter\n",
    "                        encoding=enc,\n",
    "                        quoting=quoting,\n",
    "                        usecols=[0, 1],\n",
    "                        on_bad_lines=\"skip\",\n",
    "                        skip_blank_lines=True,\n",
    "                        header=0\n",
    "                    )\n",
    "                    df.columns = [\"image\", \"label\"]\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    tried.append((enc, quoting, str(e)))\n",
    "                    df = None\n",
    "            if df is not None:\n",
    "                break\n",
    "        if df is None:\n",
    "            raise RuntimeError(f\"Could not parse labels file. Tries: {tried}\")\n",
    "\n",
    "    # 3) Clean up invisible characters\n",
    "    for col in (\"image\", \"label\"):\n",
    "        df[col] = (\n",
    "            df[col].astype(str)\n",
    "                   .str.replace(\"\\ufeff\",\"\", regex=False)  # BOM\n",
    "                   .str.replace(\"\\xa0\",\" \", regex=False)   # NBSP\n",
    "                   .str.strip()\n",
    "        )\n",
    "\n",
    "    # 4) If any cell looks like ZIP bytes (Excel), re-read as Excel\n",
    "    if df[\"label\"].str.contains(r\"PK\\x03|\\b_rels/\\.rels\\b\").any():\n",
    "        df = pd.read_excel(path, usecols=[0, 1])\n",
    "        df.columns = [\"image\", \"label\"]\n",
    "        for col in (\"image\", \"label\"):\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ------- Use it -------\n",
    "df = read_labels_safely(CSV_PATH)\n",
    "\n",
    "# Validate labels and map to IDs\n",
    "df[\"label_norm\"] = df[\"label\"].apply(normalize_label)   # will raise if anything is off\n",
    "df[\"label_id\"]    = df[\"label_norm\"].map(CLASS_TO_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "086b6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 5 rows have no matching image. Showing a few:\n",
      "                image    label\n",
      "0   A02F10-JTP-4231HA    Happy\n",
      "11    A03F7-JTPC-4438  Neutral\n",
      "70   A12F5-JTPWC-6533  Disgust\n",
      "71   A12F5-JTPWC-6554      Sad\n",
      "72   A12F5-JTPWC-6557      Sad\n",
      "Saved: C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\\splits\\train.csv 79\n",
      "Saved: C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\\splits\\val.csv 17\n",
      "Saved: C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\\splits\\test.csv 18\n",
      "\n",
      "Train distribution:\n",
      " label_norm\n",
      "happy       21\n",
      "sad         14\n",
      "anger        5\n",
      "neutral     17\n",
      "disgust      8\n",
      "surprise     8\n",
      "fear         6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Suggested class weights:\n",
      " {'happy': np.float64(0.5374149659863946), 'sad': np.float64(0.8061224489795918), 'anger': np.float64(2.257142857142857), 'neutral': np.float64(0.6638655462184874), 'disgust': np.float64(1.4107142857142858), 'surprise': np.float64(1.4107142857142858), 'fear': np.float64(1.880952380952381)}\n",
      "\n",
      "Found 18 unlabeled images.\n",
      "Copied 18 files to: C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\\unlabeled\n",
      "\n",
      "All done. Ready for training with the CSVs in: C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\\splits\n"
     ]
    }
   ],
   "source": [
    "IMAGES_DIR = Path(r\"C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\images_renamed\")\n",
    "CSV_PATH    = Path(r\"C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\TIF_labels.xlsx\")\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\")     # where to save the splits/CSVs\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MOVE_UNLABELED = False  # False = copy (safe), True = move (destructive)\n",
    "\n",
    "# Allowed image extensions:\n",
    "ALLOWED_EXTS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"]\n",
    "\n",
    "# Classes (must match your task and stay stable across runs)\n",
    "ORDERED_CLASSES = [\"happy\", \"sad\", \"anger\", \"neutral\", \"disgust\", \"surprise\", \"fear\"]\n",
    "CLASS_TO_ID = {c: i for i, c in enumerate(ORDERED_CLASSES)}\n",
    "\n",
    "# Canonical label mapping (normalize casing/variants)\n",
    "CANONICAL = {\n",
    "    \"happy\": \"happy\",\n",
    "    \"sad\": \"sad\",\n",
    "    \"anger\": \"anger\",\n",
    "    \"angry\": \"anger\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"disgust\": \"disgust\",\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"surprised\": \"surprise\",\n",
    "    \"fear\": \"fear\",\n",
    "    \"scared\": \"fear\",\n",
    "}\n",
    "# ============================================\n",
    "\n",
    "\n",
    "def normalize_label(x: str) -> str:\n",
    "    key = re.sub(r\"\\s+\", \"\", str(x).strip().lower())\n",
    "    if key not in CANONICAL:\n",
    "        raise ValueError(f\"Unknown label '{x}' (normalized='{key}'). \"\n",
    "                         f\"Allowed: {sorted(set(CANONICAL.values()))}\")\n",
    "    return CANONICAL[key]\n",
    "\n",
    "\n",
    "def read_labels_safely(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load first two columns as ['image','label'] from CSV or Excel, handling messy cases.\"\"\"\n",
    "    path = Path(path)\n",
    "\n",
    "    # Excel? (xlsx/xls are ZIPs)\n",
    "    if path.suffix.lower() in {\".xlsx\", \".xls\"} or zipfile.is_zipfile(path):\n",
    "        try:\n",
    "            df = pd.read_excel(path, usecols=[0, 1])\n",
    "            df.columns = [\"image\", \"label\"]\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read Excel file: {path}\\n{e}\")\n",
    "\n",
    "    # Try robust CSV reads with various encodings & quoting modes\n",
    "    tried = []\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
    "        for quoting in (csv.QUOTE_MINIMAL, csv.QUOTE_NONE):\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    path,\n",
    "                    engine=\"python\",   # tolerant parser\n",
    "                    sep=None,          # auto-detect delimiter\n",
    "                    encoding=enc,\n",
    "                    quoting=quoting,\n",
    "                    usecols=[0, 1],\n",
    "                    on_bad_lines=\"skip\",\n",
    "                    skip_blank_lines=True,\n",
    "                    header=0,\n",
    "                )\n",
    "                df.columns = [\"image\", \"label\"]\n",
    "                # Clean invisible characters\n",
    "                for col in (\"image\", \"label\"):\n",
    "                    df[col] = (\n",
    "                        df[col].astype(str)\n",
    "                        .str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "                        .str.replace(\"\\xa0\", \" \", regex=False)\n",
    "                        .str.strip()\n",
    "                    )\n",
    "                # If someone saved Excel as .csv by mistake and we read binary, bail out\n",
    "                if df[\"label\"].astype(str).str.contains(r\"PK\\x03|_rels/.rels\", regex=True).any():\n",
    "                    raise ValueError(\"Looks like an Excel workbook bytes leaked into CSV; use read_excel.\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                tried.append((enc, quoting, str(e)))\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"Could not parse labels file: {path}\\nTried: {tried}\")\n",
    "\n",
    "\n",
    "def build_file_indices(images_dir: Path):\n",
    "    \"\"\"Index files in IMAGES_DIR for fast resolution. Non-recursive (single folder).\"\"\"\n",
    "    files = [p for p in images_dir.iterdir() if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]\n",
    "    index_by_name = {p.name: p for p in files}                     # exact filename\n",
    "    index_by_name_ci = {p.name.lower(): p for p in files}          # case-insensitive\n",
    "    index_by_stem = {p.stem: p for p in files}                     # stem -> path\n",
    "    index_by_stem_ci = {p.stem.lower(): p for p in files}\n",
    "    # Handle potential \"two trailing chars mismatch\" between CSV ID and actual stem\n",
    "    index_by_stem_trim2 = {p.stem[:-2]: p for p in files if len(p.stem) >= 2}\n",
    "    index_by_stem_trim2_ci = {k.lower(): v for k, v in index_by_stem_trim2.items()}\n",
    "\n",
    "    return {\n",
    "        \"files\": files,\n",
    "        \"by_name\": index_by_name,\n",
    "        \"by_name_ci\": index_by_name_ci,\n",
    "        \"by_stem\": index_by_stem,\n",
    "        \"by_stem_ci\": index_by_stem_ci,\n",
    "        \"by_stem_trim2\": index_by_stem_trim2,\n",
    "        \"by_stem_trim2_ci\": index_by_stem_trim2_ci,\n",
    "    }\n",
    "\n",
    "\n",
    "def resolve_image_path_factory(indices: dict):\n",
    "    files = indices[\"files\"]\n",
    "    by_name = indices[\"by_name\"]\n",
    "    by_name_ci = indices[\"by_name_ci\"]\n",
    "    by_stem = indices[\"by_stem\"]\n",
    "    by_stem_ci = indices[\"by_stem_ci\"]\n",
    "    by_stem_trim2 = indices[\"by_stem_trim2\"]\n",
    "    by_stem_trim2_ci = indices[\"by_stem_trim2_ci\"]\n",
    "\n",
    "    def resolve_image_path(img_id: str) -> str | None:\n",
    "        img_id = str(img_id).strip()\n",
    "        if not img_id:\n",
    "            return None\n",
    "\n",
    "        # If CSV provides extension -> try exact and case-insensitive\n",
    "        if Path(img_id).suffix:\n",
    "            p = by_name.get(img_id) or by_name_ci.get(img_id.lower())\n",
    "            return str(p) if p else None\n",
    "\n",
    "        # No extension: try stem exact / ci\n",
    "        p = by_stem.get(img_id) or by_stem_ci.get(img_id.lower())\n",
    "        if p:\n",
    "            return str(p)\n",
    "\n",
    "        # Try adding common extensions via full name lookup\n",
    "        for ext in ALLOWED_EXTS:\n",
    "            name = img_id + ext\n",
    "            p = by_name.get(name) or by_name_ci.get(name.lower())\n",
    "            if p:\n",
    "                return str(p)\n",
    "\n",
    "        # Handle \"last-2-chars mismatch\" case\n",
    "        p = by_stem_trim2.get(img_id) or by_stem_trim2_ci.get(img_id.lower())\n",
    "        if p:\n",
    "            return str(p)\n",
    "\n",
    "        # Last resort: prefix match on stems (can be slow if many files)\n",
    "        lower_id = img_id.lower()\n",
    "        for cand in files:\n",
    "            if cand.stem.lower().startswith(lower_id):\n",
    "                return str(cand)\n",
    "\n",
    "        return None\n",
    "\n",
    "    return resolve_image_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) Load labels\n",
    "    df = read_labels_safely(CSV_PATH)\n",
    "\n",
    "    # 2) Normalize labels and map to IDs\n",
    "    df[\"label_norm\"] = df[\"label\"].apply(normalize_label)\n",
    "    df[\"label_id\"] = df[\"label_norm\"].map(CLASS_TO_ID)\n",
    "\n",
    "    # 3) Resolve image file paths\n",
    "    indices = build_file_indices(IMAGES_DIR)\n",
    "    resolve_image_path = resolve_image_path_factory(indices)\n",
    "    df[\"resolved_path\"] = df[\"image\"].apply(resolve_image_path)\n",
    "\n",
    "    # 4) Report and drop rows that didn't resolve to a file\n",
    "    missing = df[\"resolved_path\"].isna().sum()\n",
    "    if missing:\n",
    "        print(f\"[WARN] {missing} rows have no matching image. Showing a few:\")\n",
    "        print(df.loc[df[\"resolved_path\"].isna(), [\"image\", \"label\"]].head(5))\n",
    "    df = df.dropna(subset=[\"resolved_path\"]).copy()\n",
    "\n",
    "    # 5) De-duplicate by actual file path (if any duplicates in labels)\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=[\"resolved_path\"]).reset_index(drop=True)\n",
    "    removed_dups = before - len(df)\n",
    "    if removed_dups:\n",
    "        print(f\"[INFO] Removed {removed_dups} duplicate rows by resolved_path\")\n",
    "\n",
    "    # 6) Stratified split 70/15/15\n",
    "    TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.70, 0.15, 0.15\n",
    "    y = df[\"label_id\"].values\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=(1 - TRAIN_RATIO), random_state=42)\n",
    "    train_idx, temp_idx = next(sss1.split(np.zeros(len(df)), y))\n",
    "    df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "    df_temp = df.iloc[temp_idx].reset_index(drop=True)\n",
    "\n",
    "    temp_y = df_temp[\"label_id\"].values\n",
    "    test_portion = TEST_RATIO / (VAL_RATIO + TEST_RATIO)  # 0.5 if 15/15\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=test_portion, random_state=42)\n",
    "    val_idx, test_idx = next(sss2.split(np.zeros(len(df_temp)), temp_y))\n",
    "    df_val = df_temp.iloc[val_idx].reset_index(drop=True)\n",
    "    df_test = df_temp.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # 7) Save split CSVs\n",
    "    cols = [\"resolved_path\", \"image\", \"label\", \"label_norm\", \"label_id\"]\n",
    "    (OUTPUT_DIR / \"splits\").mkdir(parents=True, exist_ok=True)\n",
    "    df_train[cols].to_csv(OUTPUT_DIR / \"splits\" / \"train.csv\", index=False, encoding=\"utf-8\")\n",
    "    df_val[cols].to_csv(OUTPUT_DIR / \"splits\" / \"val.csv\", index=False, encoding=\"utf-8\")\n",
    "    df_test[cols].to_csv(OUTPUT_DIR / \"splits\" / \"test.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved:\", OUTPUT_DIR / \"splits\" / \"train.csv\", len(df_train))\n",
    "    print(\"Saved:\", OUTPUT_DIR / \"splits\" / \"val.csv\", len(df_val))\n",
    "    print(\"Saved:\", OUTPUT_DIR / \"splits\" / \"test.csv\", len(df_test))\n",
    "\n",
    "    # 8) Class distribution + suggested class weights\n",
    "    counts = df_train[\"label_norm\"].value_counts().reindex(ORDERED_CLASSES, fill_value=0)\n",
    "    total = counts.sum()\n",
    "    weights = {c: (total / (len(ORDERED_CLASSES) * max(1, counts[c]))) for c in ORDERED_CLASSES}\n",
    "    pd.DataFrame({\"count\": counts, \"class_weight\": pd.Series(weights)}).to_csv(\n",
    "        OUTPUT_DIR / \"train_class_stats.csv\"\n",
    "    )\n",
    "    print(\"\\nTrain distribution:\\n\", counts)\n",
    "    print(\"\\nSuggested class weights:\\n\", weights)\n",
    "\n",
    "    # 9) Collect unlabeled images (present on disk but not referenced by labels)\n",
    "    UNLABELED_DIR = OUTPUT_DIR / \"unlabeled\"\n",
    "    UNLABELED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # We index only the top-level folder; keep consistent here too:\n",
    "    all_imgs = [p for p in IMAGES_DIR.iterdir() if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]\n",
    "    labeled_paths = set(Path(p).resolve() for p in df[\"resolved_path\"].astype(str).map(Path))\n",
    "    unlabeled = [p for p in all_imgs if p.resolve() not in labeled_paths]\n",
    "\n",
    "    print(f\"\\nFound {len(unlabeled)} unlabeled images.\")\n",
    "    moved_or_copied = 0\n",
    "    for src in unlabeled:\n",
    "        dst = UNLABELED_DIR / src.name\n",
    "        i = 1\n",
    "        while dst.exists():\n",
    "            dst = UNLABELED_DIR / f\"{src.stem}_{i}{src.suffix}\"\n",
    "            i += 1\n",
    "        if MOVE_UNLABELED:\n",
    "            shutil.move(str(src), str(dst))\n",
    "        else:\n",
    "            shutil.copy2(str(src), str(dst))\n",
    "        moved_or_copied += 1\n",
    "    print(f\"{'Moved' if MOVE_UNLABELED else 'Copied'} {moved_or_copied} files to: {UNLABELED_DIR}\")\n",
    "\n",
    "    print(\"\\nAll done. Ready for training with the CSVs in:\", OUTPUT_DIR / \"splits\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9dfd0b",
   "metadata": {},
   "source": [
    "### count how many pictures are in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e9ef3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Immediate folders ==\n",
      "     0  C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\\splits\n",
      "TOTAL (immediate): 0\n",
      "\n",
      "== Recursive (every folder) ==\n",
      "TOTAL (recursive): 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\ADMIN\\Downloads\\emotion classification\\Data\\data_emotions\\splits\")  # <-- change this to your top folder\n",
    "ALLOWED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "SAVE_CSV = True  # set False if you don't want a CSV file\n",
    "CSV_PATH = ROOT / \"image_counts.csv\"\n",
    "\n",
    "def is_image(p: Path) -> bool:\n",
    "    return p.is_file() and p.suffix.lower() in ALLOWED_EXTS\n",
    "\n",
    "\n",
    "print(\"== Immediate folders ==\")\n",
    "immediate_rows = []\n",
    "targets = [ROOT] + [p for p in ROOT.iterdir() if p.is_dir()]\n",
    "for d in targets:\n",
    "    count = sum(1 for p in d.iterdir() if is_image(p))\n",
    "    immediate_rows.append((str(d), count))\n",
    "for folder, count in sorted(immediate_rows, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{count:6}  {folder}\")\n",
    "print(f\"TOTAL (immediate): {sum(c for _, c in immediate_rows)}\\n\")\n",
    "\n",
    "# 2) Recursive: count for EVERY folder in the tree\n",
    "print(\"== Recursive (every folder) ==\")\n",
    "counts = Counter()\n",
    "for p in ROOT.rglob(\"*\"):\n",
    "    if is_image(p):\n",
    "        counts[str(p.parent)] += 1\n",
    "\n",
    "recursive_rows = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for folder, count in recursive_rows:\n",
    "    print(f\"{count:6}  {folder}\")\n",
    "print(f\"TOTAL (recursive): {sum(counts.values())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec729531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
